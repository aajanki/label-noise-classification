{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label-noise robust regression\n",
    "\n",
    "The class labels in the training data are commonly assumed to be perfectly correct. This is not always a justified assumption. Humans, who have assigned the labels, may have made mistakes for example, because the assignment is not obvious or because the labeler is not an expert on the task.\n",
    "\n",
    "It is possible to model labeling mistakes by altering the classifier. This notebook explores one way to extend a multiclass logistic regression classifier to account for class-dependent label noise. The classifier and label flipping probabilities are learned jointly. We'll evaluate the robust regression model on simulated data where we inject a controlled amount of label-noise.\n",
    "\n",
    "This is based on the [Label-Noise Robust Logistic Regression and Its Applications](https://www.cs.bham.ac.uk/~axk/ecml2012.pdf) paper by Bootkrajang and Kabán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pystan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's evaluate robust regression model on the well know iris data set. The data consists of measurements of physical properties of three iris subspecies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=iris['target'],\n",
    "                                                    random_state=1234)\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=iris['feature_names'])\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=iris['feature_names'])\n",
    "\n",
    "print(f'Train sample dim: {X_train.shape}')\n",
    "print(f'Train class proportions: {iris[\"target_names\"][0]}: {(y_train == 0).mean():.2f}, {iris[\"target_names\"][1]}: {(y_train == 1).mean():.2f}, {iris[\"target_names\"][2]}: {(y_train == 2).mean():.2f}')\n",
    "print()\n",
    "print(f'Test samples: {len(y_test)}')\n",
    "print(f'Test class proportions: {iris[\"target_names\"][0]}: {(y_test == 0).mean():.2f}, {iris[\"target_names\"][1]}: {(y_test == 1).mean():.2f}, {iris[\"target_names\"][2]}: {(y_test == 2).mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard logistic regression\n",
    "\n",
    "The text book logistic regression model assumes that the observed label has been generated by a categorical logit distribution.\n",
    "\n",
    "We'll employ a weak L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=10)\n",
    "logreg.fit(X_train, y_train)\n",
    "ypred_train = logreg.predict(X_train)\n",
    "ypred_test = logreg.predict(X_test)\n",
    "\n",
    "print(f'intercept = {logreg.intercept_}')\n",
    "print(f'beta = {logreg.coef_}')\n",
    "\n",
    "print()\n",
    "print('Train')\n",
    "print(classification_report(y_train, ypred_train, target_names=iris['target_names']))\n",
    "\n",
    "print()\n",
    "print('Test')\n",
    "print(classification_report(y_test, ypred_test, target_names=iris['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust logistic regression\n",
    "\n",
    "The robust logistic regression variant by Bootkrajang and Kabán assumes that the observed labels are corrupted versions of the true labels. The true labels are latent variables generated by the standard logistic regression model. The latent labels are then corrupted by a noise process that flips a label to another with a class-dependent probability. The flip probabilities can be asymmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines and compiles the robust regression probabilistic as a Stan model.\n",
    "\n",
    "We apply the same L2 regularization as before on the regression weights and a Dirichlet prior on the flip probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N;  // number of observations\n",
    "    int<lower=1> D;  // number of predictors\n",
    "    int<lower=2> K;  // number of classes\n",
    "    matrix[N, D] X;\n",
    "    int<lower=1,upper=K> y[N];  // observed (noisy) class labels\n",
    "    real prior_sigma;\n",
    "    real transition_prior;\n",
    "}\n",
    "parameters {\n",
    "    vector[K] alpha;\n",
    "    matrix[D, K] beta;\n",
    "    simplex[K] gamma[K];\n",
    "}\n",
    "model {\n",
    "    matrix[N, K] x_beta = rep_matrix(to_row_vector(alpha), N) + X * beta;\n",
    "    \n",
    "    alpha ~ normal(0, prior_sigma);\n",
    "    to_vector(beta) ~ normal(0, prior_sigma);\n",
    "    for (i in 1:K) {\n",
    "        gamma[i] ~ dirichlet(rep_vector(transition_prior, K));\n",
    "    }\n",
    "\n",
    "    for (n in 1:N) {\n",
    "        vector[K] u = rep_vector(0.0, K);\n",
    "        vector[K] w = softmax(x_beta[n]');\n",
    "        \n",
    "        for (j in 1:K) {\n",
    "            u += w[j] * gamma[j];\n",
    "        }\n",
    "                \n",
    "        y[n] ~ categorical(u);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "sm = pystan.StanModel(model_code=robust_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a class that wraps a Stan model into a scikit-learn-like interface. Having an uniform interface simplifies comparison against scikit-learn-provided classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanClassifierWrapper():\n",
    "    def __init__(self, stan_model, prior_sigma=1, transition_prior=1, restarts=1):\n",
    "        self.sm = stan_model\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "        self.gamma_ = None\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.transition_prior = transition_prior\n",
    "        self.restarts = restarts\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        K = len(np.unique(y))\n",
    "        data = {\n",
    "            'N': X.shape[0],\n",
    "            'D': X.shape[1],\n",
    "            'K': K,\n",
    "            'X': X,\n",
    "            'y': y + 1,\n",
    "            'prior_sigma': self.prior_sigma,\n",
    "            'transition_prior': self.transition_prior\n",
    "        }\n",
    "        \n",
    "        best_par = None\n",
    "        best_lp = None\n",
    "        for _ in range(max(self.restarts, 1)):\n",
    "            res = sm.optimizing(data=data, init=self.param_initializer(K, X.shape[1]), as_vector=False)\n",
    "            if best_lp is None or res['value'] > best_lp:\n",
    "                best_par = res['par']\n",
    "                best_lp = res['value']\n",
    "            \n",
    "        self.intercept_ = best_par['alpha']\n",
    "        self.coef_ = best_par['beta'].T\n",
    "        if 'gamma' in best_par:\n",
    "            self.gamma_ = best_par['gamma']\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.asarray((self.intercept_ + X.dot(self.coef_.T)).idxmax(axis=1))\n",
    "    \n",
    "    def param_initializer(self, num_classes, num_features):\n",
    "        def wrapped():\n",
    "            ginit = 0.01/(num_classes-1)*np.ones((num_classes, num_classes))\n",
    "            np.fill_diagonal(ginit, 0.99)\n",
    "            return {\n",
    "                'alpha': np.random.uniform(-2, 2, num_classes),\n",
    "                'beta': np.random.uniform(-2, 2, (num_features, num_classes)),\n",
    "                'gamma': ginit\n",
    "            }\n",
    "        \n",
    "        return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StanClassifierWrapper(sm, prior_sigma=10, transition_prior=2, restarts=6)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "ypred_train = clf.predict(X_train)\n",
    "ypred_test = clf.predict(X_test)\n",
    "\n",
    "print(f'intercept = {clf.intercept_}')\n",
    "print(f'beta = {clf.coef_}')\n",
    "print(f'gamma = {clf.gamma_}')\n",
    "\n",
    "print()\n",
    "print('Train')\n",
    "print(classification_report(y_train, ypred_train, target_names=iris['target_names']))\n",
    "\n",
    "print()\n",
    "print('Test')\n",
    "print(classification_report(y_test, ypred_test, target_names=iris['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on simulated data\n",
    "\n",
    "We validate the impact of the robustness modeling on simulated data. The robust and standard logistic regression models are compared on different levels of artificially injected label noise.\n",
    "\n",
    "In real applications, acquiring a test set with clean labels can be a considerable challenge, because the same noise process that has corrupted the training dataset is likely to affect the test set, too. However, a testset with clean labels is necessary for fair evaluation of generalization performance. With artificially injected noise we don't have such challenges with regards to the testset. Noise is only inject on the training labels and the trained model is evaluated on noise-free test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_transition_prob(size):\n",
    "    U = np.zeros([size, size])\n",
    "    for i in range(size):\n",
    "        ind = [x for x in range(size) if x != i]\n",
    "        U[i, ind] = np.random.dirichlet(0.5*np.ones(size - 1))\n",
    "    return U\n",
    "\n",
    "def draw_noisy_labels(y_true, noise_prob, transition_prob):\n",
    "    K = transition_prob.shape[0]\n",
    "    N = len(y_true)\n",
    "\n",
    "    y_noisy = np.array(y_true)\n",
    "    for i in np.random.choice(N, size=int(noise_prob*N), replace=False):\n",
    "        y_noisy[i] = np.random.choice(range(K), p=transition_prob[y_true[i], :])\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "def eval_test_error(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    return 1 - accuracy_score(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 40\n",
    "noise_proportions = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "\n",
    "errors = []\n",
    "for i, flip_prob in enumerate(noise_proportions):\n",
    "    scores = []\n",
    "    for _ in range(n_repeats):\n",
    "        T = draw_transition_prob(3)\n",
    "        y_noisy = draw_noisy_labels(y_train, flip_prob, T)\n",
    "        noise_level = 1.0 - (y_noisy == y_train).mean()\n",
    "\n",
    "        robust = StanClassifierWrapper(sm, prior_sigma=10, transition_prior=2, restarts=6)\n",
    "        r_err = eval_test_error(robust, X_train, y_noisy, X_test, y_test)\n",
    "        errors.append({\n",
    "            'Model': 'Robust regression',\n",
    "            'Noise level (%)': 100*noise_level,\n",
    "            'Test error (%)': 100*r_err\n",
    "        })\n",
    "        \n",
    "        logreg = LogisticRegression(C=10)\n",
    "        r_err = eval_test_error(logreg, X_train, y_noisy, X_test, y_test)\n",
    "        errors.append({\n",
    "            'Model': 'Logistic regression',\n",
    "            'Noise level (%)': 100*noise_level,\n",
    "            'Test error (%)': 100*r_err\n",
    "        })\n",
    "\n",
    "errors = pd.DataFrame(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot displays the accuracy of the two models on the test set at different noise levels.\n",
    "\n",
    "The robust model performs very close to the noise-free condition when 10% of labels are changed. The performance of the standard logistic regression model has already deteriorated quite much at the same amount of noise. Even when 30% of labels are changed to be incorrect, the robust classifier attains test error below 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='Noise level (%)', y='Test error (%)', hue='Model', kind='line', data=errors, height=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
